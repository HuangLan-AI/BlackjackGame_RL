{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e481005c-745f-49e7-beff-ab69ab5b6044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7fa91a47-5d83-4757-96aa-dc0a8b78d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the 'src' directory to the system path if it's not already\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'environment')))\n",
    "\n",
    "import random\n",
    "from environment import BlackjackGame  # Adjust if your environment file is in a different location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2fd25e68-d48b-4c5b-9058-49f5506662ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDifference:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, lambd=0.9):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.lambd = lambd\n",
    "\n",
    "        # Q-table as a dictionary\n",
    "        self.Q = {}  # Key: state (tuple), Value: [Q-value for action 0, Q-value for action 1]\n",
    "        self.E = {}  # Eligibility trace dictionary\n",
    "\n",
    "        print(\"Dynamic Q-table initialized with dictionary structure.\")\n",
    "\n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"\n",
    "        Get the Q-values for a given state. Default to [0, 0] if not present.\n",
    "        \"\"\"\n",
    "        return self.Q.get(tuple(state), [0.0, 0.0])\n",
    "\n",
    "    def set_q_value(self, state, action, q_value):\n",
    "        \"\"\"\n",
    "        Set the Q-value for a specific state-action pair.\n",
    "        \"\"\"\n",
    "        # Ensure the state exists in the Q-table\n",
    "        if tuple(state) not in self.Q:\n",
    "            self.Q[tuple(state)] = [0.0, 0.0]\n",
    "        self.Q[tuple(state)][action] = q_value\n",
    "\n",
    "    def get_e_trace(self, state):\n",
    "        \"\"\"\n",
    "        Get the eligibility trace for a given state. Default to [0, 0] if not present.\n",
    "        \"\"\"\n",
    "        return self.E.get(tuple(state), [0.0, 0.0])\n",
    "\n",
    "    def set_e_trace(self, state, action, value):\n",
    "        \"\"\"\n",
    "        Set the eligibility trace for a specific state-action pair.\n",
    "        \"\"\"\n",
    "        # Ensure the state exists in the eligibility trace\n",
    "        if tuple(state) not in self.E:\n",
    "            self.E[tuple(state)] = [0.0, 0.0]\n",
    "        self.E[tuple(state)][action] = value\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy policy for action selection.\n",
    "        Randomly selects an action if the state is not in Q-table.\n",
    "        \"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(self.env.actions)\n",
    "        else:\n",
    "            q_values = self.get_q_values(state)\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def train(self, num_episodes, true_count=0, on_policy=True):\n",
    "        \"\"\"\n",
    "        Train the agent using Temporal Difference learning with eligibility traces.\n",
    "        \"\"\"\n",
    "        self.env.reset(true_count=true_count)\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            self.E.clear()  # Reset eligibility trace\n",
    "\n",
    "            reward, state, winner = self.env.new_game()\n",
    "            while winner:\n",
    "                reward, state, winner = self.env.new_game()  # Skip initial game if it's over immediately\n",
    "\n",
    "            action = self.epsilon_greedy_policy(state)\n",
    "            while not self.env.winner:\n",
    "                reward, next_state, winner = self.env.step(action)\n",
    "                next_action = self.epsilon_greedy_policy(next_state)\n",
    "\n",
    "                # Compute target\n",
    "                next_q_values = self.get_q_values(next_state)\n",
    "                if winner:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    if on_policy:\n",
    "                        target = reward + self.gamma * next_q_values[next_action]\n",
    "                    else:\n",
    "                        target = reward + self.gamma * max(next_q_values)\n",
    "\n",
    "                # Update Q-value and eligibility trace\n",
    "                current_q_value = self.get_q_values(state)[action]\n",
    "                delta = target - current_q_value\n",
    "\n",
    "                # Update eligibility trace\n",
    "                e_trace = self.get_e_trace(state)\n",
    "                e_trace[action] += 1\n",
    "                self.set_e_trace(state, action, e_trace[action])\n",
    "\n",
    "                # Update Q-values and decay traces\n",
    "                for s, trace in self.E.items():\n",
    "                    q_values = self.get_q_values(s)\n",
    "                    for a in range(len(q_values)):\n",
    "                        updated_q_value = q_values[a] + self.alpha * delta * trace[a]\n",
    "                        self.set_q_value(s, a, updated_q_value)\n",
    "                        self.set_e_trace(s, a, trace[a] * self.gamma * self.lambd)\n",
    "\n",
    "                if winner:\n",
    "                    break  # End of episode\n",
    "\n",
    "                state, action = next_state, next_action\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns the best action for a given state based on the trained Q-values.\n",
    "        For unseen states, it chooses a random action.\n",
    "        \"\"\"\n",
    "        q_values = self.get_q_values(state)  # Retrieve Q-values for the state\n",
    "        if q_values == [0.0, 0.0]:  # If unseen state, Q-values will be default\n",
    "            return random.choice(self.env.actions)\n",
    "        return int(np.argmax(q_values))\n",
    "\n",
    "    def print_q_table(self):\n",
    "        \"\"\"\n",
    "        Print the current Q-table for debugging purposes.\n",
    "        \"\"\"\n",
    "        print(\"Current Q-table:\")\n",
    "        for state, q_values in self.Q.items():\n",
    "            print(f\"State: {state}, Q-values: {q_values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "20e81083-66e1-4189-be6a-9e8ac26e36a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Q-table initialized with dictionary structure.\n"
     ]
    }
   ],
   "source": [
    "env = BlackjackGame()\n",
    "agent = TemporalDifference(\n",
    "    env=env,\n",
    "    alpha=0.1,    # Learning rate\n",
    "    gamma=0.9,    # Discount factor\n",
    "    epsilon=0.1,  # Exploration rate\n",
    "    lambd=1     # Trace decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "622eaa39-5d2c-4cc6-af49-c7cadca478d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 10000000/10000000 [05:28<00:00, 30433.63it/s]\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10000000  # Define the number of episodes to train\n",
    "agent.train(num_episodes=num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a35098f3-9819-46c4-aecf-0c67e14ded36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13347"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0766c04a-19ca-4df9-bb19-1ac520e2d1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Games: 100%|█████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 3004.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 1000 test games:\n",
      "Win Rate: 27.90%\n",
      "Loss Rate: 70.10%\n",
      "Tie Rate: 2.00%\n",
      "Blackjack Rate: 6.40%\n",
      "Total Reward: -7800.0\n",
      "Average Reward: -7.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_test_games = 1000\n",
    "wins = 0\n",
    "losses = 0\n",
    "ties = 0\n",
    "blackjack_count = 0\n",
    "total_reward = 0\n",
    "env = BlackjackGame()\n",
    "\n",
    "for _ in tqdm(range(num_test_games), desc=\"Testing Games\"):\n",
    "    env.reset(true_count=2)\n",
    "    game_reward, state, winner = env.new_game()  # Start a new game\n",
    "\n",
    "    while not winner:\n",
    "        # Get the best action based on the trained model\n",
    "        action = agent.get_best_action(state)\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        reward, next_state, winner = env.step(action)\n",
    "        \n",
    "        # Accumulate reward for the current game\n",
    "        game_reward += reward\n",
    "        \n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "    # Track total reward\n",
    "    total_reward += game_reward\n",
    "\n",
    "    # Step 3: Record the outcome\n",
    "    if winner == 'player':\n",
    "        wins += 1\n",
    "    elif winner == 'blackjack':\n",
    "        wins += 1\n",
    "        blackjack_count += 1\n",
    "    elif winner == 'dealer':\n",
    "        losses += 1\n",
    "    else:  # Tie\n",
    "        ties += 1\n",
    "\n",
    "# Step 4: Compute performance metrics\n",
    "win_rate = wins / num_test_games\n",
    "loss_rate = losses / num_test_games\n",
    "tie_rate = ties / num_test_games\n",
    "average_reward = total_reward / num_test_games\n",
    "blackjack_rate = blackjack_count / num_test_games\n",
    "\n",
    "# Step 5: Print results\n",
    "print(f\"Results after {num_test_games} test games:\")\n",
    "print(f\"Win Rate: {win_rate:.2%}\")\n",
    "print(f\"Loss Rate: {loss_rate:.2%}\")\n",
    "print(f\"Tie Rate: {tie_rate:.2%}\")\n",
    "print(f\"Blackjack Rate: {blackjack_rate:.2%}\")\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "print(f\"Average Reward: {average_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232301aa-2eb5-457f-8ebb-cb68ea9bdec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL Env 3.11",
   "language": "python",
   "name": "rl-venv-3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
