{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath('../environment'))\n",
    "sys.path.append(os.path.abspath('../model'))\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Import the environment and agent\n",
    "from environment_gym import BlackjackGameGym\n",
    "from utilities import test_ppo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log directory\n",
    "log_dir = \"./logs/ppo/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_grid = {\n",
    "    \"learning_rate\": [3e-4, 1e-4, 5e-5],  \n",
    "    \"batch_size\": [32, 64, 128],\n",
    "    \"gamma\": [0.95, 0.99],\n",
    "    \"n_epochs\": [10, 20],\n",
    "    \"clip_range\": [0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Generate all hyperparameter combinations\n",
    "param_combinations = list(itertools.product(*param_grid.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training Model 1/72 with params: (0.0003, 32, 0.95, 10, 0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishock/.local/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=20000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Average Reward over 10000 episodes: -1.79\n",
      "Win Rate: 41.47%\n",
      "Loss Rate: 50.42%\n",
      "Tie Rate: 8.11%\n",
      "\n",
      " Results for Model 0:\n",
      "Hyperparameters: (0.0003, 32, 0.95, 10, 0.1)\n",
      "Win Rate: 41.47%\n",
      "Loss Rate: 50.42%\n",
      "Tie Rate: 8.11%\n",
      "Average Reward: -1.79\n",
      "\n",
      " Training Model 2/72 with params: (0.0003, 32, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.45\n",
      "Win Rate: 42.52%\n",
      "Loss Rate: 49.76%\n",
      "Tie Rate: 7.72%\n",
      "\n",
      " Results for Model 1:\n",
      "Hyperparameters: (0.0003, 32, 0.95, 10, 0.2)\n",
      "Win Rate: 42.52%\n",
      "Loss Rate: 49.76%\n",
      "Tie Rate: 7.72%\n",
      "Average Reward: -1.45\n",
      "\n",
      " Training Model 3/72 with params: (0.0003, 32, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=0.20 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-4.20 +/- 14.92\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=3.60 +/- 15.08\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Average Reward over 10000 episodes: -1.65\n",
      "Win Rate: 42.16%\n",
      "Loss Rate: 50.39%\n",
      "Tie Rate: 7.45%\n",
      "\n",
      " Results for Model 2:\n",
      "Hyperparameters: (0.0003, 32, 0.95, 20, 0.1)\n",
      "Win Rate: 42.16%\n",
      "Loss Rate: 50.39%\n",
      "Tie Rate: 7.45%\n",
      "Average Reward: -1.65\n",
      "\n",
      " Training Model 4/72 with params: (0.0003, 32, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=80000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=4.00 +/- 8.05\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-12.20 +/- 9.56\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.52\n",
      "Win Rate: 42.49%\n",
      "Loss Rate: 50.09%\n",
      "Tie Rate: 7.42%\n",
      "\n",
      " Results for Model 3:\n",
      "Hyperparameters: (0.0003, 32, 0.95, 20, 0.2)\n",
      "Win Rate: 42.49%\n",
      "Loss Rate: 50.09%\n",
      "Tie Rate: 7.42%\n",
      "Average Reward: -1.52\n",
      "\n",
      " Training Model 5/72 with params: (0.0003, 32, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=100000, episode_reward=7.80 +/- 9.99\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-4.40 +/- 14.87\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=-7.80 +/- 16.10\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-4.00 +/- 8.05\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=20.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Average Reward over 10000 episodes: -1.14\n",
      "Win Rate: 43.20%\n",
      "Loss Rate: 48.91%\n",
      "Tie Rate: 7.89%\n",
      "\n",
      " Results for Model 4:\n",
      "Hyperparameters: (0.0003, 32, 0.99, 10, 0.1)\n",
      "Win Rate: 43.20%\n",
      "Loss Rate: 48.91%\n",
      "Tie Rate: 7.89%\n",
      "Average Reward: -1.14\n",
      "\n",
      " Training Model 6/72 with params: (0.0003, 32, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 14.97\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-16.00 +/- 8.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -2.06\n",
      "Win Rate: 41.26%\n",
      "Loss Rate: 51.54%\n",
      "Tie Rate: 7.20%\n",
      "\n",
      " Results for Model 5:\n",
      "Hyperparameters: (0.0003, 32, 0.99, 10, 0.2)\n",
      "Win Rate: 41.26%\n",
      "Loss Rate: 51.54%\n",
      "Tie Rate: 7.20%\n",
      "Average Reward: -2.06\n",
      "\n",
      " Training Model 7/72 with params: (0.0003, 32, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 2.60 +/- 1.02\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.22\n",
      "Win Rate: 42.90%\n",
      "Loss Rate: 49.02%\n",
      "Tie Rate: 8.08%\n",
      "\n",
      " Results for Model 6:\n",
      "Hyperparameters: (0.0003, 32, 0.99, 20, 0.1)\n",
      "Win Rate: 42.90%\n",
      "Loss Rate: 49.02%\n",
      "Tie Rate: 8.08%\n",
      "Average Reward: -1.22\n",
      "\n",
      " Training Model 8/72 with params: (0.0003, 32, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 14.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.20 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=4.20 +/- 14.92\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-4.00 +/- 8.05\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 8.02\n",
      "Episode length: 2.00 +/- 0.63\n",
      "Average Reward over 10000 episodes: -1.46\n",
      "Win Rate: 41.85%\n",
      "Loss Rate: 49.13%\n",
      "Tie Rate: 9.02%\n",
      "\n",
      " Results for Model 7:\n",
      "Hyperparameters: (0.0003, 32, 0.99, 20, 0.2)\n",
      "Win Rate: 41.85%\n",
      "Loss Rate: 49.13%\n",
      "Tie Rate: 9.02%\n",
      "Average Reward: -1.46\n",
      "\n",
      " Training Model 9/72 with params: (0.0003, 64, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.60 +/- 12.66\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.73\n",
      "Win Rate: 41.84%\n",
      "Loss Rate: 50.47%\n",
      "Tie Rate: 7.69%\n",
      "\n",
      " Results for Model 8:\n",
      "Hyperparameters: (0.0003, 64, 0.95, 10, 0.1)\n",
      "Win Rate: 41.84%\n",
      "Loss Rate: 50.47%\n",
      "Tie Rate: 7.69%\n",
      "Average Reward: -1.73\n",
      "\n",
      " Training Model 10/72 with params: (0.0003, 64, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 14.97\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 1.20\n",
      "Eval num_timesteps=80000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Average Reward over 10000 episodes: -1.26\n",
      "Win Rate: 42.72%\n",
      "Loss Rate: 49.01%\n",
      "Tie Rate: 8.27%\n",
      "\n",
      " Results for Model 9:\n",
      "Hyperparameters: (0.0003, 64, 0.95, 10, 0.2)\n",
      "Win Rate: 42.72%\n",
      "Loss Rate: 49.01%\n",
      "Tie Rate: 8.27%\n",
      "Average Reward: -1.26\n",
      "\n",
      " Training Model 11/72 with params: (0.0003, 64, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-3.60 +/- 15.08\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 2.00 +/- 1.55\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.32\n",
      "Win Rate: 42.87%\n",
      "Loss Rate: 49.46%\n",
      "Tie Rate: 7.67%\n",
      "\n",
      " Results for Model 10:\n",
      "Hyperparameters: (0.0003, 64, 0.95, 20, 0.1)\n",
      "Win Rate: 42.87%\n",
      "Loss Rate: 49.46%\n",
      "Tie Rate: 7.67%\n",
      "Average Reward: -1.32\n",
      "\n",
      " Training Model 12/72 with params: (0.0003, 64, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-11.80 +/- 10.05\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=60000, episode_reward=4.00 +/- 14.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=7.80 +/- 16.10\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=4.00 +/- 14.97\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.41\n",
      "Win Rate: 42.12%\n",
      "Loss Rate: 49.15%\n",
      "Tie Rate: 8.73%\n",
      "\n",
      " Results for Model 11:\n",
      "Hyperparameters: (0.0003, 64, 0.95, 20, 0.2)\n",
      "Win Rate: 42.12%\n",
      "Loss Rate: 49.15%\n",
      "Tie Rate: 8.73%\n",
      "Average Reward: -1.41\n",
      "\n",
      " Training Model 13/72 with params: (0.0003, 64, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=3.60 +/- 15.08\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-3.60 +/- 8.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-7.80 +/- 16.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Average Reward over 10000 episodes: -1.43\n",
      "Win Rate: 42.64%\n",
      "Loss Rate: 49.79%\n",
      "Tie Rate: 7.57%\n",
      "\n",
      " Results for Model 12:\n",
      "Hyperparameters: (0.0003, 64, 0.99, 10, 0.1)\n",
      "Win Rate: 42.64%\n",
      "Loss Rate: 49.79%\n",
      "Tie Rate: 7.57%\n",
      "Average Reward: -1.43\n",
      "\n",
      " Training Model 14/72 with params: (0.0003, 64, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-4.80 +/- 7.60\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=100000, episode_reward=-8.20 +/- 9.66\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.78\n",
      "Win Rate: 41.54%\n",
      "Loss Rate: 50.45%\n",
      "Tie Rate: 8.01%\n",
      "\n",
      " Results for Model 13:\n",
      "Hyperparameters: (0.0003, 64, 0.99, 10, 0.2)\n",
      "Win Rate: 41.54%\n",
      "Loss Rate: 50.45%\n",
      "Tie Rate: 8.01%\n",
      "Average Reward: -1.78\n",
      "\n",
      " Training Model 15/72 with params: (0.0003, 64, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-4.40 +/- 14.87\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=180000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.32\n",
      "Win Rate: 42.91%\n",
      "Loss Rate: 49.52%\n",
      "Tie Rate: 7.57%\n",
      "\n",
      " Results for Model 14:\n",
      "Hyperparameters: (0.0003, 64, 0.99, 20, 0.1)\n",
      "Win Rate: 42.91%\n",
      "Loss Rate: 49.52%\n",
      "Tie Rate: 7.57%\n",
      "Average Reward: -1.32\n",
      "\n",
      " Training Model 16/72 with params: (0.0003, 64, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-4.60 +/- 7.71\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=-4.40 +/- 14.87\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=-12.00 +/- 9.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 8.05\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.21\n",
      "Win Rate: 42.74%\n",
      "Loss Rate: 48.79%\n",
      "Tie Rate: 8.47%\n",
      "\n",
      " Results for Model 15:\n",
      "Hyperparameters: (0.0003, 64, 0.99, 20, 0.2)\n",
      "Win Rate: 42.74%\n",
      "Loss Rate: 48.79%\n",
      "Tie Rate: 8.47%\n",
      "Average Reward: -1.21\n",
      "\n",
      " Training Model 17/72 with params: (0.0003, 128, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.46\n",
      "Win Rate: 42.10%\n",
      "Loss Rate: 49.41%\n",
      "Tie Rate: 8.49%\n",
      "\n",
      " Results for Model 16:\n",
      "Hyperparameters: (0.0003, 128, 0.95, 10, 0.1)\n",
      "Win Rate: 42.10%\n",
      "Loss Rate: 49.41%\n",
      "Tie Rate: 8.49%\n",
      "Average Reward: -1.46\n",
      "\n",
      " Training Model 18/72 with params: (0.0003, 128, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-3.60 +/- 15.08\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=3.60 +/- 8.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=4.60 +/- 7.71\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 12.65\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.69\n",
      "Win Rate: 41.91%\n",
      "Loss Rate: 50.38%\n",
      "Tie Rate: 7.71%\n",
      "\n",
      " Results for Model 17:\n",
      "Hyperparameters: (0.0003, 128, 0.95, 10, 0.2)\n",
      "Win Rate: 41.91%\n",
      "Loss Rate: 50.38%\n",
      "Tie Rate: 7.71%\n",
      "Average Reward: -1.69\n",
      "\n",
      " Training Model 19/72 with params: (0.0003, 128, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 1.20\n",
      "Eval num_timesteps=100000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=3.80 +/- 15.03\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.11\n",
      "Win Rate: 43.38%\n",
      "Loss Rate: 48.94%\n",
      "Tie Rate: 7.68%\n",
      "\n",
      " Results for Model 18:\n",
      "Hyperparameters: (0.0003, 128, 0.95, 20, 0.1)\n",
      "Win Rate: 43.38%\n",
      "Loss Rate: 48.94%\n",
      "Tie Rate: 7.68%\n",
      "Average Reward: -1.11\n",
      "\n",
      " Training Model 20/72 with params: (0.0003, 128, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=12.00 +/- 9.82\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.47\n",
      "Win Rate: 42.01%\n",
      "Loss Rate: 49.35%\n",
      "Tie Rate: 8.64%\n",
      "\n",
      " Results for Model 19:\n",
      "Hyperparameters: (0.0003, 128, 0.95, 20, 0.2)\n",
      "Win Rate: 42.01%\n",
      "Loss Rate: 49.35%\n",
      "Tie Rate: 8.64%\n",
      "Average Reward: -1.47\n",
      "\n",
      " Training Model 21/72 with params: (0.0003, 128, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=3.60 +/- 15.08\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 8.05\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-12.20 +/- 9.56\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.40 +/- 0.49\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 2.20 +/- 1.17\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.42\n",
      "Win Rate: 42.25%\n",
      "Loss Rate: 49.33%\n",
      "Tie Rate: 8.42%\n",
      "\n",
      " Results for Model 20:\n",
      "Hyperparameters: (0.0003, 128, 0.99, 10, 0.1)\n",
      "Win Rate: 42.25%\n",
      "Loss Rate: 49.33%\n",
      "Tie Rate: 8.42%\n",
      "Average Reward: -1.42\n",
      "\n",
      " Training Model 22/72 with params: (0.0003, 128, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-8.60 +/- 9.31\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 12.66\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=3.40 +/- 8.31\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=3.60 +/- 8.24\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.57\n",
      "Win Rate: 42.05%\n",
      "Loss Rate: 49.88%\n",
      "Tie Rate: 8.07%\n",
      "\n",
      " Results for Model 21:\n",
      "Hyperparameters: (0.0003, 128, 0.99, 10, 0.2)\n",
      "Win Rate: 42.05%\n",
      "Loss Rate: 49.88%\n",
      "Tie Rate: 8.07%\n",
      "Average Reward: -1.57\n",
      "\n",
      " Training Model 23/72 with params: (0.0003, 128, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-16.20 +/- 7.60\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=3.80 +/- 8.13\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 2.20 +/- 1.17\n",
      "Eval num_timesteps=180000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Average Reward over 10000 episodes: -1.37\n",
      "Win Rate: 42.31%\n",
      "Loss Rate: 49.16%\n",
      "Tie Rate: 8.53%\n",
      "\n",
      " Results for Model 22:\n",
      "Hyperparameters: (0.0003, 128, 0.99, 20, 0.1)\n",
      "Win Rate: 42.31%\n",
      "Loss Rate: 49.16%\n",
      "Tie Rate: 8.53%\n",
      "Average Reward: -1.37\n",
      "\n",
      " Training Model 24/72 with params: (0.0003, 128, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-4.40 +/- 14.87\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=7.80 +/- 9.99\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=-4.00 +/- 8.05\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-3.60 +/- 8.24\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.12\n",
      "Win Rate: 43.37%\n",
      "Loss Rate: 48.98%\n",
      "Tie Rate: 7.65%\n",
      "\n",
      " Results for Model 23:\n",
      "Hyperparameters: (0.0003, 128, 0.99, 20, 0.2)\n",
      "Win Rate: 43.37%\n",
      "Loss Rate: 48.98%\n",
      "Tie Rate: 7.65%\n",
      "Average Reward: -1.12\n",
      "\n",
      " Training Model 25/72 with params: (0.0001, 32, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.60 +/- 12.66\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-3.60 +/- 15.08\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=100000, episode_reward=-8.60 +/- 9.31\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -2.04\n",
      "Win Rate: 41.08%\n",
      "Loss Rate: 51.28%\n",
      "Tie Rate: 7.64%\n",
      "\n",
      " Results for Model 24:\n",
      "Hyperparameters: (0.0001, 32, 0.95, 10, 0.1)\n",
      "Win Rate: 41.08%\n",
      "Loss Rate: 51.28%\n",
      "Tie Rate: 7.64%\n",
      "Average Reward: -2.04\n",
      "\n",
      " Training Model 26/72 with params: (0.0001, 32, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=3.80 +/- 8.13\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=3.80 +/- 8.13\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -2.01\n",
      "Win Rate: 41.20%\n",
      "Loss Rate: 51.26%\n",
      "Tie Rate: 7.54%\n",
      "\n",
      " Results for Model 25:\n",
      "Hyperparameters: (0.0001, 32, 0.95, 10, 0.2)\n",
      "Win Rate: 41.20%\n",
      "Loss Rate: 51.26%\n",
      "Tie Rate: 7.54%\n",
      "Average Reward: -2.01\n",
      "\n",
      " Training Model 27/72 with params: (0.0001, 32, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 12.66\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-8.40 +/- 9.48\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=140000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.75\n",
      "Episode length: 1.80 +/- 1.17\n",
      "Average Reward over 10000 episodes: -1.45\n",
      "Win Rate: 42.28%\n",
      "Loss Rate: 49.55%\n",
      "Tie Rate: 8.17%\n",
      "\n",
      " Results for Model 26:\n",
      "Hyperparameters: (0.0001, 32, 0.95, 20, 0.1)\n",
      "Win Rate: 42.28%\n",
      "Loss Rate: 49.55%\n",
      "Tie Rate: 8.17%\n",
      "Average Reward: -1.45\n",
      "\n",
      " Training Model 28/72 with params: (0.0001, 32, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 2.00 +/- 0.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.71\n",
      "Win Rate: 41.74%\n",
      "Loss Rate: 50.29%\n",
      "Tie Rate: 7.97%\n",
      "\n",
      " Results for Model 27:\n",
      "Hyperparameters: (0.0001, 32, 0.95, 20, 0.2)\n",
      "Win Rate: 41.74%\n",
      "Loss Rate: 50.29%\n",
      "Tie Rate: 7.97%\n",
      "Average Reward: -1.71\n",
      "\n",
      " Training Model 29/72 with params: (0.0001, 32, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 2.00 +/- 0.63\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.89\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 50.79%\n",
      "Tie Rate: 7.85%\n",
      "\n",
      " Results for Model 28:\n",
      "Hyperparameters: (0.0001, 32, 0.99, 10, 0.1)\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 50.79%\n",
      "Tie Rate: 7.85%\n",
      "Average Reward: -1.89\n",
      "\n",
      " Training Model 30/72 with params: (0.0001, 32, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.40 +/- 12.66\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-12.00 +/- 9.82\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=60000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-4.60 +/- 7.71\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.60\n",
      "Win Rate: 42.16%\n",
      "Loss Rate: 50.16%\n",
      "Tie Rate: 7.68%\n",
      "\n",
      " Results for Model 29:\n",
      "Hyperparameters: (0.0001, 32, 0.99, 10, 0.2)\n",
      "Win Rate: 42.16%\n",
      "Loss Rate: 50.16%\n",
      "Tie Rate: 7.68%\n",
      "Average Reward: -1.60\n",
      "\n",
      " Training Model 31/72 with params: (0.0001, 32, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 2.00 +/- 0.89\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.19\n",
      "Win Rate: 42.72%\n",
      "Loss Rate: 48.65%\n",
      "Tie Rate: 8.63%\n",
      "\n",
      " Results for Model 30:\n",
      "Hyperparameters: (0.0001, 32, 0.99, 20, 0.1)\n",
      "Win Rate: 42.72%\n",
      "Loss Rate: 48.65%\n",
      "Tie Rate: 8.63%\n",
      "Average Reward: -1.19\n",
      "\n",
      " Training Model 32/72 with params: (0.0001, 32, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 9.82\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=3.60 +/- 8.24\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.05\n",
      "Win Rate: 43.14%\n",
      "Loss Rate: 48.40%\n",
      "Tie Rate: 8.46%\n",
      "\n",
      " Results for Model 31:\n",
      "Hyperparameters: (0.0001, 32, 0.99, 20, 0.2)\n",
      "Win Rate: 43.14%\n",
      "Loss Rate: 48.40%\n",
      "Tie Rate: 8.46%\n",
      "Average Reward: -1.05\n",
      "\n",
      " Training Model 33/72 with params: (0.0001, 64, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-4.20 +/- 14.92\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 14.97\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-3.80 +/- 15.03\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.66\n",
      "Win Rate: 41.89%\n",
      "Loss Rate: 50.18%\n",
      "Tie Rate: 7.93%\n",
      "\n",
      " Results for Model 32:\n",
      "Hyperparameters: (0.0001, 64, 0.95, 10, 0.1)\n",
      "Win Rate: 41.89%\n",
      "Loss Rate: 50.18%\n",
      "Tie Rate: 7.93%\n",
      "Average Reward: -1.66\n",
      "\n",
      " Training Model 34/72 with params: (0.0001, 64, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=8.20 +/- 15.90\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=4.40 +/- 14.87\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 8.05\n",
      "Episode length: 2.40 +/- 1.50\n",
      "Eval num_timesteps=100000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.81\n",
      "Win Rate: 41.90%\n",
      "Loss Rate: 50.96%\n",
      "Tie Rate: 7.14%\n",
      "\n",
      " Results for Model 33:\n",
      "Hyperparameters: (0.0001, 64, 0.95, 10, 0.2)\n",
      "Win Rate: 41.90%\n",
      "Loss Rate: 50.96%\n",
      "Tie Rate: 7.14%\n",
      "Average Reward: -1.81\n",
      "\n",
      " Training Model 35/72 with params: (0.0001, 64, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 12.66\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 12.65\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Average Reward over 10000 episodes: -1.46\n",
      "Win Rate: 42.24%\n",
      "Loss Rate: 49.56%\n",
      "Tie Rate: 8.20%\n",
      "\n",
      " Results for Model 34:\n",
      "Hyperparameters: (0.0001, 64, 0.95, 20, 0.1)\n",
      "Win Rate: 42.24%\n",
      "Loss Rate: 49.56%\n",
      "Tie Rate: 8.20%\n",
      "Average Reward: -1.46\n",
      "\n",
      " Training Model 36/72 with params: (0.0001, 64, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=3.60 +/- 8.24\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=7.80 +/- 9.99\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-4.20 +/- 7.93\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=3.20 +/- 8.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=3.80 +/- 15.03\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.57\n",
      "Win Rate: 42.19%\n",
      "Loss Rate: 50.02%\n",
      "Tie Rate: 7.79%\n",
      "\n",
      " Results for Model 35:\n",
      "Hyperparameters: (0.0001, 64, 0.95, 20, 0.2)\n",
      "Win Rate: 42.19%\n",
      "Loss Rate: 50.02%\n",
      "Tie Rate: 7.79%\n",
      "Average Reward: -1.57\n",
      "\n",
      " Training Model 37/72 with params: (0.0001, 64, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.71\n",
      "Win Rate: 41.86%\n",
      "Loss Rate: 50.40%\n",
      "Tie Rate: 7.74%\n",
      "\n",
      " Results for Model 36:\n",
      "Hyperparameters: (0.0001, 64, 0.99, 10, 0.1)\n",
      "Win Rate: 41.86%\n",
      "Loss Rate: 50.40%\n",
      "Tie Rate: 7.74%\n",
      "Average Reward: -1.71\n",
      "\n",
      " Training Model 38/72 with params: (0.0001, 64, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=3.40 +/- 8.31\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.84\n",
      "Win Rate: 41.50%\n",
      "Loss Rate: 50.69%\n",
      "Tie Rate: 7.81%\n",
      "\n",
      " Results for Model 37:\n",
      "Hyperparameters: (0.0001, 64, 0.99, 10, 0.2)\n",
      "Win Rate: 41.50%\n",
      "Loss Rate: 50.69%\n",
      "Tie Rate: 7.81%\n",
      "Average Reward: -1.84\n",
      "\n",
      " Training Model 39/72 with params: (0.0001, 64, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.80 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.95\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 51.09%\n",
      "Tie Rate: 7.55%\n",
      "\n",
      " Results for Model 38:\n",
      "Hyperparameters: (0.0001, 64, 0.99, 20, 0.1)\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 51.09%\n",
      "Tie Rate: 7.55%\n",
      "Average Reward: -1.95\n",
      "\n",
      " Training Model 40/72 with params: (0.0001, 64, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.20 +/- 14.92\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Average Reward over 10000 episodes: -1.36\n",
      "Win Rate: 42.55%\n",
      "Loss Rate: 49.35%\n",
      "Tie Rate: 8.10%\n",
      "\n",
      " Results for Model 39:\n",
      "Hyperparameters: (0.0001, 64, 0.99, 20, 0.2)\n",
      "Win Rate: 42.55%\n",
      "Loss Rate: 49.35%\n",
      "Tie Rate: 8.10%\n",
      "Average Reward: -1.36\n",
      "\n",
      " Training Model 41/72 with params: (0.0001, 128, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-16.20 +/- 7.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-7.80 +/- 16.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=3.60 +/- 8.24\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -2.17\n",
      "Win Rate: 41.02%\n",
      "Loss Rate: 51.85%\n",
      "Tie Rate: 7.13%\n",
      "\n",
      " Results for Model 40:\n",
      "Hyperparameters: (0.0001, 128, 0.95, 10, 0.1)\n",
      "Win Rate: 41.02%\n",
      "Loss Rate: 51.85%\n",
      "Tie Rate: 7.13%\n",
      "Average Reward: -2.17\n",
      "\n",
      " Training Model 42/72 with params: (0.0001, 128, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-4.00 +/- 8.05\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.77\n",
      "Win Rate: 41.75%\n",
      "Loss Rate: 50.62%\n",
      "Tie Rate: 7.63%\n",
      "\n",
      " Results for Model 41:\n",
      "Hyperparameters: (0.0001, 128, 0.95, 10, 0.2)\n",
      "Win Rate: 41.75%\n",
      "Loss Rate: 50.62%\n",
      "Tie Rate: 7.63%\n",
      "Average Reward: -1.77\n",
      "\n",
      " Training Model 43/72 with params: (0.0001, 128, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=3.80 +/- 8.13\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=-4.40 +/- 14.87\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=8.00 +/- 9.82\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.48\n",
      "Win Rate: 42.43%\n",
      "Loss Rate: 49.84%\n",
      "Tie Rate: 7.73%\n",
      "\n",
      " Results for Model 42:\n",
      "Hyperparameters: (0.0001, 128, 0.95, 20, 0.1)\n",
      "Win Rate: 42.43%\n",
      "Loss Rate: 49.84%\n",
      "Tie Rate: 7.73%\n",
      "Average Reward: -1.48\n",
      "\n",
      " Training Model 44/72 with params: (0.0001, 128, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.20 +/- 14.92\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.86\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 50.65%\n",
      "Tie Rate: 7.99%\n",
      "\n",
      " Results for Model 43:\n",
      "Hyperparameters: (0.0001, 128, 0.95, 20, 0.2)\n",
      "Win Rate: 41.36%\n",
      "Loss Rate: 50.65%\n",
      "Tie Rate: 7.99%\n",
      "Average Reward: -1.86\n",
      "\n",
      " Training Model 45/72 with params: (0.0001, 128, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=80000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 2.20 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -2.01\n",
      "Win Rate: 41.23%\n",
      "Loss Rate: 51.26%\n",
      "Tie Rate: 7.51%\n",
      "\n",
      " Results for Model 44:\n",
      "Hyperparameters: (0.0001, 128, 0.99, 10, 0.1)\n",
      "Win Rate: 41.23%\n",
      "Loss Rate: 51.26%\n",
      "Tie Rate: 7.51%\n",
      "Average Reward: -2.01\n",
      "\n",
      " Training Model 46/72 with params: (0.0001, 128, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-7.80 +/- 9.97\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=7.80 +/- 16.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 17.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-8.00 +/- 9.82\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -2.29\n",
      "Win Rate: 40.57%\n",
      "Loss Rate: 52.02%\n",
      "Tie Rate: 7.41%\n",
      "\n",
      " Results for Model 45:\n",
      "Hyperparameters: (0.0001, 128, 0.99, 10, 0.2)\n",
      "Win Rate: 40.57%\n",
      "Loss Rate: 52.02%\n",
      "Tie Rate: 7.41%\n",
      "Average Reward: -2.29\n",
      "\n",
      " Training Model 47/72 with params: (0.0001, 128, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 2.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.60 +/- 7.71\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.60 +/- 1.20\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=4.00 +/- 8.05\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.63\n",
      "Win Rate: 41.71%\n",
      "Loss Rate: 49.87%\n",
      "Tie Rate: 8.42%\n",
      "\n",
      " Results for Model 46:\n",
      "Hyperparameters: (0.0001, 128, 0.99, 20, 0.1)\n",
      "Win Rate: 41.71%\n",
      "Loss Rate: 49.87%\n",
      "Tie Rate: 8.42%\n",
      "Average Reward: -1.63\n",
      "\n",
      " Training Model 48/72 with params: (0.0001, 128, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 2.20 +/- 0.75\n",
      "Eval num_timesteps=120000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.49\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.86\n",
      "Win Rate: 41.48%\n",
      "Loss Rate: 50.77%\n",
      "Tie Rate: 7.75%\n",
      "\n",
      " Results for Model 47:\n",
      "Hyperparameters: (0.0001, 128, 0.99, 20, 0.2)\n",
      "Win Rate: 41.48%\n",
      "Loss Rate: 50.77%\n",
      "Tie Rate: 7.75%\n",
      "Average Reward: -1.86\n",
      "\n",
      " Training Model 49/72 with params: (5e-05, 32, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=100000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=20.00 +/- 0.00\n",
      "Episode length: 2.00 +/- 0.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-12.20 +/- 9.56\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.71\n",
      "Win Rate: 41.72%\n",
      "Loss Rate: 50.29%\n",
      "Tie Rate: 7.99%\n",
      "\n",
      " Results for Model 48:\n",
      "Hyperparameters: (5e-05, 32, 0.95, 10, 0.1)\n",
      "Win Rate: 41.72%\n",
      "Loss Rate: 50.29%\n",
      "Tie Rate: 7.99%\n",
      "Average Reward: -1.71\n",
      "\n",
      " Training Model 50/72 with params: (5e-05, 32, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=60000, episode_reward=7.80 +/- 16.10\n",
      "Episode length: 1.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 2.00 +/- 1.26\n",
      "Eval num_timesteps=140000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.75\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.64\n",
      "Win Rate: 41.99%\n",
      "Loss Rate: 50.20%\n",
      "Tie Rate: 7.81%\n",
      "\n",
      " Results for Model 49:\n",
      "Hyperparameters: (5e-05, 32, 0.95, 10, 0.2)\n",
      "Win Rate: 41.99%\n",
      "Loss Rate: 50.20%\n",
      "Tie Rate: 7.81%\n",
      "Average Reward: -1.64\n",
      "\n",
      " Training Model 51/72 with params: (5e-05, 32, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.98\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 14.98\n",
      "Episode length: 2.00 +/- 1.10\n",
      "Eval num_timesteps=140000, episode_reward=-16.00 +/- 8.00\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-3.80 +/- 8.13\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.99\n",
      "Win Rate: 41.11%\n",
      "Loss Rate: 51.06%\n",
      "Tie Rate: 7.83%\n",
      "\n",
      " Results for Model 50:\n",
      "Hyperparameters: (5e-05, 32, 0.95, 20, 0.1)\n",
      "Win Rate: 41.11%\n",
      "Loss Rate: 51.06%\n",
      "Tie Rate: 7.83%\n",
      "Average Reward: -1.99\n",
      "\n",
      " Training Model 52/72 with params: (5e-05, 32, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-0.20 +/- 12.67\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=4.20 +/- 14.92\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-8.60 +/- 9.31\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=-4.20 +/- 7.93\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-7.80 +/- 9.99\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.70\n",
      "Win Rate: 41.92%\n",
      "Loss Rate: 50.43%\n",
      "Tie Rate: 7.65%\n",
      "\n",
      " Results for Model 51:\n",
      "Hyperparameters: (5e-05, 32, 0.95, 20, 0.2)\n",
      "Win Rate: 41.92%\n",
      "Loss Rate: 50.43%\n",
      "Tie Rate: 7.65%\n",
      "Average Reward: -1.70\n",
      "\n",
      " Training Model 53/72 with params: (5e-05, 32, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-20.00 +/- 0.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.40 +/- 12.66\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=180000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.82\n",
      "Win Rate: 41.66%\n",
      "Loss Rate: 50.76%\n",
      "Tie Rate: 7.58%\n",
      "\n",
      " Results for Model 52:\n",
      "Hyperparameters: (5e-05, 32, 0.99, 10, 0.1)\n",
      "Win Rate: 41.66%\n",
      "Loss Rate: 50.76%\n",
      "Tie Rate: 7.58%\n",
      "Average Reward: -1.82\n",
      "\n",
      " Training Model 54/72 with params: (5e-05, 32, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -2.11\n",
      "Win Rate: 40.96%\n",
      "Loss Rate: 51.53%\n",
      "Tie Rate: 7.51%\n",
      "\n",
      " Results for Model 53:\n",
      "Hyperparameters: (5e-05, 32, 0.99, 10, 0.2)\n",
      "Win Rate: 40.96%\n",
      "Loss Rate: 51.53%\n",
      "Tie Rate: 7.51%\n",
      "Average Reward: -2.11\n",
      "\n",
      " Training Model 55/72 with params: (5e-05, 32, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-8.60 +/- 9.31\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Average Reward over 10000 episodes: -1.63\n",
      "Win Rate: 41.87%\n",
      "Loss Rate: 50.01%\n",
      "Tie Rate: 8.12%\n",
      "\n",
      " Results for Model 54:\n",
      "Hyperparameters: (5e-05, 32, 0.99, 20, 0.1)\n",
      "Win Rate: 41.87%\n",
      "Loss Rate: 50.01%\n",
      "Tie Rate: 8.12%\n",
      "Average Reward: -1.63\n",
      "\n",
      " Training Model 56/72 with params: (5e-05, 32, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.75\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.68\n",
      "Win Rate: 41.85%\n",
      "Loss Rate: 50.27%\n",
      "Tie Rate: 7.88%\n",
      "\n",
      " Results for Model 55:\n",
      "Hyperparameters: (5e-05, 32, 0.99, 20, 0.2)\n",
      "Win Rate: 41.85%\n",
      "Loss Rate: 50.27%\n",
      "Tie Rate: 7.88%\n",
      "Average Reward: -1.68\n",
      "\n",
      " Training Model 57/72 with params: (5e-05, 64, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.80 +/- 1.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=4.20 +/- 7.93\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.61\n",
      "Win Rate: 42.04%\n",
      "Loss Rate: 50.11%\n",
      "Tie Rate: 7.85%\n",
      "\n",
      " Results for Model 56:\n",
      "Hyperparameters: (5e-05, 64, 0.95, 10, 0.1)\n",
      "Win Rate: 42.04%\n",
      "Loss Rate: 50.11%\n",
      "Tie Rate: 7.85%\n",
      "Average Reward: -1.61\n",
      "\n",
      " Training Model 58/72 with params: (5e-05, 64, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-12.20 +/- 9.56\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.75\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.97\n",
      "Win Rate: 41.10%\n",
      "Loss Rate: 50.94%\n",
      "Tie Rate: 7.96%\n",
      "\n",
      " Results for Model 57:\n",
      "Hyperparameters: (5e-05, 64, 0.95, 10, 0.2)\n",
      "Win Rate: 41.10%\n",
      "Loss Rate: 50.94%\n",
      "Tie Rate: 7.96%\n",
      "Average Reward: -1.97\n",
      "\n",
      " Training Model 59/72 with params: (5e-05, 64, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=200000, episode_reward=-0.40 +/- 0.49\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.70\n",
      "Win Rate: 41.79%\n",
      "Loss Rate: 50.31%\n",
      "Tie Rate: 7.90%\n",
      "\n",
      " Results for Model 58:\n",
      "Hyperparameters: (5e-05, 64, 0.95, 20, 0.1)\n",
      "Win Rate: 41.79%\n",
      "Loss Rate: 50.31%\n",
      "Tie Rate: 7.90%\n",
      "Average Reward: -1.70\n",
      "\n",
      " Training Model 60/72 with params: (5e-05, 64, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=0.20 +/- 12.67\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=8.00 +/- 9.82\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=1.00 +/- 0.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=4.60 +/- 7.71\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.97\n",
      "Win Rate: 41.06%\n",
      "Loss Rate: 50.92%\n",
      "Tie Rate: 8.02%\n",
      "\n",
      " Results for Model 59:\n",
      "Hyperparameters: (5e-05, 64, 0.95, 20, 0.2)\n",
      "Win Rate: 41.06%\n",
      "Loss Rate: 50.92%\n",
      "Tie Rate: 8.02%\n",
      "Average Reward: -1.97\n",
      "\n",
      " Training Model 61/72 with params: (5e-05, 64, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 14.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=4.00 +/- 14.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Average Reward over 10000 episodes: -2.09\n",
      "Win Rate: 41.30%\n",
      "Loss Rate: 51.75%\n",
      "Tie Rate: 6.95%\n",
      "\n",
      " Results for Model 60:\n",
      "Hyperparameters: (5e-05, 64, 0.99, 10, 0.1)\n",
      "Win Rate: 41.30%\n",
      "Loss Rate: 51.75%\n",
      "Tie Rate: 6.95%\n",
      "Average Reward: -2.09\n",
      "\n",
      " Training Model 62/72 with params: (5e-05, 64, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-16.00 +/- 8.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 14.97\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.74\n",
      "Win Rate: 41.90%\n",
      "Loss Rate: 50.62%\n",
      "Tie Rate: 7.48%\n",
      "\n",
      " Results for Model 61:\n",
      "Hyperparameters: (5e-05, 64, 0.99, 10, 0.2)\n",
      "Win Rate: 41.90%\n",
      "Loss Rate: 50.62%\n",
      "Tie Rate: 7.48%\n",
      "Average Reward: -1.74\n",
      "\n",
      " Training Model 63/72 with params: (5e-05, 64, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-3.60 +/- 8.24\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=60000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-12.40 +/- 9.31\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.95\n",
      "Win Rate: 41.01%\n",
      "Loss Rate: 50.76%\n",
      "Tie Rate: 8.23%\n",
      "\n",
      " Results for Model 62:\n",
      "Hyperparameters: (5e-05, 64, 0.99, 20, 0.1)\n",
      "Win Rate: 41.01%\n",
      "Loss Rate: 50.76%\n",
      "Tie Rate: 8.23%\n",
      "Average Reward: -1.95\n",
      "\n",
      " Training Model 64/72 with params: (5e-05, 64, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-3.80 +/- 8.11\n",
      "Episode length: 1.60 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 14.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.40 +/- 12.66\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=120000, episode_reward=3.60 +/- 15.08\n",
      "Episode length: 1.80 +/- 0.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=200000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.93\n",
      "Win Rate: 41.35%\n",
      "Loss Rate: 50.99%\n",
      "Tie Rate: 7.66%\n",
      "\n",
      " Results for Model 63:\n",
      "Hyperparameters: (5e-05, 64, 0.99, 20, 0.2)\n",
      "Win Rate: 41.35%\n",
      "Loss Rate: 50.99%\n",
      "Tie Rate: 7.66%\n",
      "Average Reward: -1.93\n",
      "\n",
      " Training Model 65/72 with params: (5e-05, 128, 0.95, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.75\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.60\n",
      "Win Rate: 42.40%\n",
      "Loss Rate: 50.41%\n",
      "Tie Rate: 7.19%\n",
      "\n",
      " Results for Model 64:\n",
      "Hyperparameters: (5e-05, 128, 0.95, 10, 0.1)\n",
      "Win Rate: 42.40%\n",
      "Loss Rate: 50.41%\n",
      "Tie Rate: 7.19%\n",
      "Average Reward: -1.60\n",
      "\n",
      " Training Model 66/72 with params: (5e-05, 128, 0.95, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-8.20 +/- 9.66\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-7.80 +/- 16.10\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 2.00 +/- 0.89\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=3.80 +/- 8.13\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 1.20\n",
      "Eval num_timesteps=180000, episode_reward=4.40 +/- 7.84\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -2.07\n",
      "Win Rate: 41.05%\n",
      "Loss Rate: 51.42%\n",
      "Tie Rate: 7.53%\n",
      "\n",
      " Results for Model 65:\n",
      "Hyperparameters: (5e-05, 128, 0.95, 10, 0.2)\n",
      "Win Rate: 41.05%\n",
      "Loss Rate: 51.42%\n",
      "Tie Rate: 7.53%\n",
      "Average Reward: -2.07\n",
      "\n",
      " Training Model 67/72 with params: (5e-05, 128, 0.95, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=7.80 +/- 16.10\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=4.00 +/- 8.05\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=160000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-7.60 +/- 10.13\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.53\n",
      "Win Rate: 42.24%\n",
      "Loss Rate: 49.90%\n",
      "Tie Rate: 7.86%\n",
      "\n",
      " Results for Model 66:\n",
      "Hyperparameters: (5e-05, 128, 0.95, 20, 0.1)\n",
      "Win Rate: 42.24%\n",
      "Loss Rate: 49.90%\n",
      "Tie Rate: 7.86%\n",
      "Average Reward: -1.53\n",
      "\n",
      " Training Model 68/72 with params: (5e-05, 128, 0.95, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=7.40 +/- 10.29\n",
      "Episode length: 1.60 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=80000, episode_reward=0.40 +/- 0.80\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=100000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=-4.40 +/- 7.84\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=160000, episode_reward=12.00 +/- 16.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-11.80 +/- 10.05\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=200000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Average Reward over 10000 episodes: -1.88\n",
      "Win Rate: 41.51%\n",
      "Loss Rate: 50.91%\n",
      "Tie Rate: 7.58%\n",
      "\n",
      " Results for Model 67:\n",
      "Hyperparameters: (5e-05, 128, 0.95, 20, 0.2)\n",
      "Win Rate: 41.51%\n",
      "Loss Rate: 50.91%\n",
      "Tie Rate: 7.58%\n",
      "Average Reward: -1.88\n",
      "\n",
      " Training Model 69/72 with params: (5e-05, 128, 0.99, 10, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-3.60 +/- 15.08\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.60 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=80000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=100000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Eval num_timesteps=120000, episode_reward=-3.60 +/- 8.21\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-0.40 +/- 0.80\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Average Reward over 10000 episodes: -1.91\n",
      "Win Rate: 41.66%\n",
      "Loss Rate: 51.19%\n",
      "Tie Rate: 7.15%\n",
      "\n",
      " Results for Model 68:\n",
      "Hyperparameters: (5e-05, 128, 0.99, 10, 0.1)\n",
      "Win Rate: 41.66%\n",
      "Loss Rate: 51.19%\n",
      "Tie Rate: 7.15%\n",
      "Average Reward: -1.91\n",
      "\n",
      " Training Model 70/72 with params: (5e-05, 128, 0.99, 10, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=8.00 +/- 16.00\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-12.00 +/- 16.00\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=100000, episode_reward=-8.20 +/- 9.66\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=120000, episode_reward=7.40 +/- 10.29\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=160000, episode_reward=-3.80 +/- 15.03\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Average Reward over 10000 episodes: -1.68\n",
      "Win Rate: 41.89%\n",
      "Loss Rate: 50.27%\n",
      "Tie Rate: 7.84%\n",
      "\n",
      " Results for Model 69:\n",
      "Hyperparameters: (5e-05, 128, 0.99, 10, 0.2)\n",
      "Win Rate: 41.89%\n",
      "Loss Rate: 50.27%\n",
      "Tie Rate: 7.84%\n",
      "Average Reward: -1.68\n",
      "\n",
      " Training Model 71/72 with params: (5e-05, 128, 0.99, 20, 0.1)\n",
      "Eval num_timesteps=20000, episode_reward=-8.20 +/- 15.90\n",
      "Episode length: 1.20 +/- 0.40\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=80000, episode_reward=-0.60 +/- 0.80\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=100000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-0.20 +/- 0.75\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=140000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=160000, episode_reward=-12.40 +/- 9.31\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=180000, episode_reward=0.00 +/- 17.89\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=200000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.60 +/- 0.80\n",
      "Average Reward over 10000 episodes: -1.68\n",
      "Win Rate: 41.91%\n",
      "Loss Rate: 50.32%\n",
      "Tie Rate: 7.77%\n",
      "\n",
      " Results for Model 70:\n",
      "Hyperparameters: (5e-05, 128, 0.99, 20, 0.1)\n",
      "Win Rate: 41.91%\n",
      "Loss Rate: 50.32%\n",
      "Tie Rate: 7.77%\n",
      "Average Reward: -1.68\n",
      "\n",
      " Training Model 72/72 with params: (5e-05, 128, 0.99, 20, 0.2)\n",
      "Eval num_timesteps=20000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=16.00 +/- 8.00\n",
      "Episode length: 1.60 +/- 1.20\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=4.00 +/- 19.60\n",
      "Episode length: 1.40 +/- 0.80\n",
      "Eval num_timesteps=80000, episode_reward=-8.00 +/- 16.00\n",
      "Episode length: 1.80 +/- 0.75\n",
      "Eval num_timesteps=100000, episode_reward=-0.80 +/- 0.40\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.89\n",
      "Episode length: 1.20 +/- 0.40\n",
      "Eval num_timesteps=140000, episode_reward=-0.20 +/- 0.98\n",
      "Episode length: 1.60 +/- 0.49\n",
      "Eval num_timesteps=160000, episode_reward=0.20 +/- 0.98\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=180000, episode_reward=0.80 +/- 0.40\n",
      "Episode length: 1.00 +/- 0.00\n",
      "Eval num_timesteps=200000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 1.40 +/- 0.49\n",
      "Average Reward over 10000 episodes: -1.62\n",
      "Win Rate: 42.32%\n",
      "Loss Rate: 50.43%\n",
      "Tie Rate: 7.25%\n",
      "\n",
      " Results for Model 71:\n",
      "Hyperparameters: (5e-05, 128, 0.99, 20, 0.2)\n",
      "Win Rate: 42.32%\n",
      "Loss Rate: 50.43%\n",
      "Tie Rate: 7.25%\n",
      "Average Reward: -1.62\n",
      "\n",
      " Best Model Hyperparameters (based on win rate):\n",
      "Hyperparameters: (0.0003, 128, 0.95, 20, 0.1)\n",
      "Best Win Rate: 43.38%\n",
      "Loss Rate: 50.43%\n",
      "Tie Rate: 7.25%\n",
      "Average Reward: -1.62\n",
      "\n",
      " Best model saved as 'ppo_best_model'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aishock/.local/lib/python3.12/site-packages/stable_baselines3/common/save_util.py:284: UserWarning: Path 'best_trained_models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "# Wrap evaluation environment\n",
    "eval_env = BlackjackGameGym()\n",
    "eval_env.reset()\n",
    "\n",
    "# Track the best model\n",
    "best_model = None\n",
    "best_model_params = None\n",
    "best_win_rate = -1\n",
    "\n",
    "# Train and evaluate for each combination\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"\\n Training Model {i+1}/{len(param_combinations)} with params: {params}\")\n",
    "\n",
    "    # Assign hyperparameters\n",
    "    learning_rate, batch_size, gamma, n_epochs, clip_range = params\n",
    "\n",
    "    # Define model with selected hyperparameters\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=BlackjackGameGym(),\n",
    "        device=\"cpu\",\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=gamma,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        clip_range=clip_range\n",
    "    )\n",
    "\n",
    "    # Evaluation callback\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir + f\"best_model{i}/\",\n",
    "        log_path=log_dir + \"results/\",\n",
    "        eval_freq=20000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    total_timesteps = 200000\n",
    "    model.learn(total_timesteps=total_timesteps, callback=eval_callback)\n",
    "\n",
    "    # Evaluate final performance\n",
    "    test_results = test_ppo_model(model, eval_env, num_test_games=10000, true_count=2)\n",
    "\n",
    "    # Print test results for this model\n",
    "    print(f\"\\n Results for Model {i}:\")\n",
    "    print(f\"Hyperparameters: {params}\")\n",
    "    print(f\"Win Rate: {test_results['win_rate'] * 100:.2f}%\")\n",
    "    print(f\"Loss Rate: {test_results['loss_rate'] * 100:.2f}%\")\n",
    "    print(f\"Tie Rate: {test_results['tie_rate'] * 100:.2f}%\")\n",
    "    print(f\"Average Reward: {test_results['average_reward']:.2f}\")\n",
    "\n",
    "    # Save model\n",
    "    model.save(log_dir + f\"ppo_model{i}\")\n",
    "\n",
    "    # Track best model based on win rate\n",
    "    if test_results[\"win_rate\"] > best_win_rate:\n",
    "        best_win_rate = test_results[\"win_rate\"]\n",
    "        best_model = model\n",
    "        best_model_params = params\n",
    "\n",
    "# Print best model parameters and results\n",
    "print(\"\\n Best Model Hyperparameters (based on win rate):\")\n",
    "print(f\"Hyperparameters: {best_model_params}\")\n",
    "print(f\"Best Win Rate: {best_win_rate * 100:.2f}%\")\n",
    "print(f\"Loss Rate: {test_results['loss_rate'] * 100:.2f}%\")\n",
    "print(f\"Tie Rate: {test_results['tie_rate'] * 100:.2f}%\")\n",
    "print(f\"Average Reward: {test_results['average_reward']:.2f}\")\n",
    "\n",
    "# Save the best model separately\n",
    "if best_model:\n",
    "    best_model_dir = \"best_trained_models\"\n",
    "    best_model.save(os.path.join(best_model_dir, \"ppo_best_model\"))\n",
    "    print(\"\\n Best model saved as 'ppo_best_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
